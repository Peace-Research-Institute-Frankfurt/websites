---
title: "Arms control and EDT – new concepts, new opportunities?"
intro: ""
order: 6
reading_time: 15
---

## From quantitative to qualitative arms control

<LectureVideo smil="/lu15/HSFK-PC_LU15-CH08_12b_250109.smil" provider="video-stream">

  How can arms control respond to the challenges stemming from this plethora of new technologies? One thing is certain – in most cases the traditional, quantitative-oriented approaches fail. In contrast to tanks or missiles, counting weapons based on new and emerging technologies is a futile exercise.

  <Figure
    size="medium"
    src="LU15_E1_inhalte_grafiken_18022025_LU_15_6 Kopie.svg"
    caption="Classical verification vs. verification of software"
    credit="Grübelfabrik, CC BY-NC-SA"
    alt="The graphic is divided into two sections. On the left side, an eye icon is positioned above five identical tank icons, each outlined in dark blue. Four of the tanks have green checkmarks next to them, indicating verification. This side represents the visual inspection of physical military equipment. On the right side, another eye icon appears above a cluster of binary numbers (ones and zeros) that fade into the background. At the center of this section is a large red question mark, symbolizing uncertainty. This side represents the challenge of verifying software code, which cannot be inspected visually like physical objects."
    license=""
  ></Figure>

  Consequently, the key lies in a qualitative rather than a quantitative approach. Take autonomy in weapons systems as an example. From an arms control perspective, the main insight here is that the good old days of treaties and regimes relying mainly on quantification for verification and compliance are over. Dual-use hardware and software is what makes these conventional weapons systems tick. And numbers are less important than capabilities.

  Exotic and hard to come by fissile materials and complicated enrichment procedures or chemical precursors and laboratories are not required to build autonomous weapons systems that can wreak havoc on an enemy force or even a civilian population in a terrorist attack; there are no warheads for an inspection team to count, maybe not even facilities or weapons systems to scrutinise in an inspection.

  In the not too distant future, the hardware of a weapon with autonomy in target selection and engagement might be 3D-printed just-in-time, and the software running it can be stolen online and enhanced using AI.

  <Figure
    size="medium"
    src="LU15_E1_inhalte_grafiken_18022025_LU_15_6 Kopie 2.svg"
    caption=""
    credit="Grübelfabrik, CC BY-NC-SA"
    alt="The graphic consists of three icons, arranged from left to right. On the left, an icon of a brain is depicted, made up of interconnected lines and nodes, symbolizing complex thinking or neural networks. In the middle, an icon of a 3D printer or laser cutting machine is shown, with a nozzle pointing downwards and a thin line extending beneath it, representing manufacturing or precision creation. On the right, an icon of a laptop is displayed, with binary code (ones and zeros) appearing on its screen, symbolizing digital processing or computer code"
    license=""
  ></Figure>

  Consequently, if regulation – that is, an internationally agreed legal instrument providing guard-rails for the use of weapon autonomy – is desirable, then specific uses rather than numbers of weapons systems need to be addressed. There is no fixed category of ‘autonomous weapons’ to be clearly delineated from ‘non-autonomous weapons’. Hence the target of regulation cannot be the hardware. Instead, it must the human–machine relationship. Who or what – human or machine – is deciding what, when, and under what circumstances. This question needs addressing in a context-dependent manner to assure that humans remain meaningfully in control of decision-making and thus legally accountable and morally responsible when deadly military force is applied.

  The same holds true for military AI in a broader sense. First, AI is a functionality, instantiated by intangible software code. Second, AI is a dual-use technology, and military AI is based on civilian developments transferred to military applications. Third, AI is not a weapon but a general enabler. A specific AI model might be problematic when applied to a specific weapons system, but entirely benign in other contexts. This will be difficult to address from an arms control perspective. Controlling AI is nevertheless important, as AI has the potential to speed up processes, reduce warning times and thus crisis stability.

  So, on a very general level, these new technologies pose a serious challenge for classical arms control thinking, especially when it comes to legally binding treaties with clear verification measures.[^1]

</LectureVideo>

On the other hand, some good ideas for arms control in the realm of EDT have been already pitched.

## Using EDT for arms control – new opportunities

While many focus only on the negative effects of emerging technologies in the military, many new technologies also have at least the potential to help with verification and support weapons inspectors. The most obvious case is the use of unmanned systems for measurements or sample collection in hazardous environments which are unsafe for humans. Small systems can access hard-to-reach places and underwater drones can operate in flooded areas. While the use of physical drones or new and more precise sensors has rather obvious advantages, arms control experts are also thinking hard about how AI could make their lives easier. In the area of arms control, AI-based processes can improve verification, i.e. the accuracy and speed at which contract breaches can be identified, and therefore deter potential fraudsters. The spectrum of possible applications ranges from the analysis of trade data to uncover clues on the proliferation of weapons of mass destruction, to the identification of landmines that is boosted by AI with improved ground-penetrating radars.[^2] Others are experimenting with AI for detecting change within satellite images, e.g. to identify the expansion of bases, the analysis of open source pictures to identify facilities that are in operation, or to support the analysis of seismic occurrences to detect unnatural events like the testing of a nuclear device.[^3] But other possibilities worth considering might be the use of translation software, improving, for example, inspectors’ abilities to evaluate and understand large amounts of relevant documents.

<Figure
  size="medium"
  src="ai-stock.jpg"
  caption=""
  credit="US Army/public domain"
  alt=""
  license=""
></Figure>

The idea of using AI as a tool for arms control is not new. In as early as 1987, a volume on ‘Arms and Artificial Intelligence’, published by SIPRI, dedicated a whole chapter to the issue.[^4] Artificial intelligence has only become more capable since then, and many pilot projects have shown that it can enhance arms control significantly.

Artificial intelligence could bring a new level of objectivity to arms control and reduce human error and bias.

In an internal poll conducted by the IAEA, 86 percent of respondents were either ‘very’ or ‘somewhat confident’ ‘about the prospect of AI and ML to help the Department in safeguards surveillance’ (IAEA 2020, p. 12).

However, since the results of AI models are highly dependent on external factors, especially the data used to train them, the use of AI in arms control raises a new issue: Can the model be ‘trusted’? Whether states adhering to an arms control regime are willing to accept analysis based on ‘opaque’ algorithms remains an open question. Most commentators therefore agree that, while AI can be a useful tool to support arms control, it should be deployed to assist human officials, not replace them.

In sum, it is obvious that arms control can be significantly improved with the use of new emerging technologies, but new technologies probably won’t revolutionise arms control. As long as the relevant actors do not trust each other, it is unlikely that new technologies will be able to compensate for that lack of trust.



[^1]: Reinhold, Thomas. 2022. “Arms Control for Artificial Intelligence”, In: Reinhold, T./Schörnig, N.: Armament, Arms Control and Artificial Intelligence. The Janus-faced Nature of Machine Learning in the Military Realm. Springer, 211–26.
[^2]: Lück, Nico. 2019. “Machine Learning-powered Artificial Intelligence in Arms Control”, PRIF-Report 8/19, https://www.prif.org/fileadmin/HSFK/hsfk_publikationen/prif0819.pdf
[^3]: Schörnig, Niklas. 2022. “Artificial Intelligence as an Arms Control Tool: Opportunities and Challenges", In: Reinhold, T./Schörnig, N.: Armament, Arms Control and Artificial Intelligence. The Janus-faced Nature of Machine Learning in the Military Realm. Springer, 57–72.
[^4]: Din, A. M. (ed.). 1987. Arms and Artificial Intelligence. Oxford University Press.
