---
title: 'Artificial intelligence and quantum computing'
order: 4
reading_time: 5
---

## Lecture video
Note: This is a sample video and will be updated later.
<LectureVideo lu="15" vl="vl1" provider="">

This is the video's transcript. It can contain **bold**, _italics_, [links](#1), and most of the components listed below.

</LectureVideo>

## Artificial intelligence


"Artificial Intelligence (AI) is the future. [...] Whoever leads in AI will rule the world." [^13]
This was the central message that President Vladimir Putin conveyed to more than one million Russian school pupils in a video call in September 2017. The announcement was no surprise: AI has become one of the most promising technologies in more recent history and the pace of progress is astounding – this holds true both for the civilian and the military realm.
Most people forget, for example, that as late as 2004, autonomous cars were unable to drive more than a couple of miles on an empty desert track. In 2005, however, five cars were able to finish the second DARPA Grand Challenge, a race for robotic cars funded by the U.S. Defense Advanced Research Projects Agency (DARPA).

[A figure is missing here!]

While we are not seeing fully autonomous cars mixing with human-steered vehicles just yet, many AI-based technologies and assistant systems have already found their way into commercial current generation vehicles. Artificial intelligence has also surpassed human capabilities in contexts where many observers were expecting human superiority for a long time to come. 1996 the computer-program Deep Blue had beaten the Chess World Maser Kasparov with a simple brute-force approach.
In 2016, Google's AlphaGo Program beat Grandmaster Lee Sedol in Go, a game significantly more complex than chess.
Another milestone was reached in August 2020. Again this was down to DARPA, pitching an AI-controlled jet fighter against a human Air Force pilot in a simulated dogfight. While the conditions were not as symmetric as in Go, the fact that AI won five to nil against the human was seen as the start of a new era by many. When it comes to the use of AI, the United States and Russia are not the only countries with a strategic interest in what artificial intelligence has to offer. Many countries have published AI strategies for the coming years and decades. [^14]


'It is no wonder that militaries worldwide are keen to implement AI to enhance their capabilities. [^15] The number of military applications for AI are vast:'
* Analysis of data collected by all kinds of sensors on the battlefield
* Identification and classification of potential targets, even camouflaged
* Enhanced automation of drones or the control of drone swarms
* Support of tactical decisions, or even optimised logistics

What is obvious from this list is that AI is widely perceived not as a particular weapons system but as an enabler – just as the combustion engine was at the beginning of the 20th century. With the US, Russia, China – and to some extent the EU – competing for AI leadership, the fear of an AI arms race does not seem too far-fetched. In any case, the use of AI in the military realm is going to increase significantly in the years to come.

### Artificial Intelligence: Milestones of the last 20 years and recent military applications
While most people think they know what AI is, it is always important to clarify what is understood by the term 'AI' in a specific context. There are two basic forms of AI. On the one hand, we have very complex 'expert systems', which can be understood as tremendously complex decision trees, where the system 'decides' based on a high number of different variables. In principle, these systems are deterministic as the same starting conditions always lead to the same result. Yet, due to the sheer complexity and number of variables, humans do have difficulty keeping up. While these systems were very common a few decades ago, modern systems use a different approach.

[A figure is missing here!]

[A figure is missing here!]

What are more common today are AI systems based on machine learning. Here the system compares large amounts of data for similarities using statistical models. Given enough pictures of, for example, cats, the system can use statistical methods to determine similarities to identify cats on new pictures without being told what to look at. Machine learning has made tremendous progress in the last couple of years and some experts today only use the term AI to refer to machine learning algorithms. Another form of AI is “deep learning”, where the computer learns by trying to replicate a neural net as in the human brain.
But machine learning AI is not without difficulties. Expert Gary Marcus, for example, came up with four characteristics of AI. [^16] Its 'greedy' (that is hungry for data), 'brittle' (fails spectacularly when confronted with untrained tasks), 'opaque' (prone to inexplicable errors and therefore difficult to debug) and 'shallow', because despite the use of the term 'deep learning', there is in fact no understanding of what has been learned.
This raises the basic question:How can one be sure about what the algorithms have actually learned? As correlation is not causation, how can AI be 'trusted'?
From a military perspective, the issue of reliability is at least as important as in the civilian sphere. Imagine an AI-powered lethal autonomous weapons system going rogue, maybe even starting a war by mistake. Some experts fear that in a war of necessity, a war where national survival is at stake, states might use untested or unverified military AI to gain superiority.
While this is indeed a potential risk, many states are at least aware of the danger of unrestricted use of unreliable AI and some suggest developing norms on which to base the military use of it.

One of the most important initiatives is the 'Political Declaration on Responsible Military Use of Artificial Intelligence and Autonomy' –  launched in early 2023 by the United States and (as of November 2023) endorsed by 47 states. [^17]  The declaration features ten measures, including, for example, the call for states to take proactive steps to minimise unintended bias; train users to sufficiently understand the capabilities and limitations of AI-powered systems; ensure that AI capabilities only have explicit, well-defined uses; and implement appropriate safeguards, e.g. the ability to deactivate a system when it shows unintended behaviour.
But an even more profound revolution in computing is in the starting blocks, one that will potentially cause even greater upheaval than machine learning already has. The technology we are talking about here is quantum computing.


[^13]: https://www.figma.com/file/7QPCYA9ZVgNGKwS7XvFaYm/%5BPRIF%5D-LU15-Storyboards?type=whiteboard&node-id=4-1515&t=EcehoLwG3HnKbJHN-4
[^14]: Galindo, L./ Perset K./ Sheeka, F. 2021. “An overview of national AI strategies and policies”, in:OECD Going Digital Toolkit Notes, no. 14, OECD Publishing, Paris, available at:https://doi.org/10.1787/c05140d9-en.
[^15]: Sauer, Frank. 2022. “The Military Rationale for AI”, In: Reinhold, T./Schörnig, N.: Armament, Arms Control and Artificial Intelligence. The Janus-faced Nature of Machine Learning in the Military Realm. Springer, 27–38.
[^16]: Marcus 2018: https://arxiv.org/ftp/arxiv/papers/1801/1801.00631.pdf
[^17]: https://www.defense.gov/News/News-Stories/Article/Article/3597093/us-endorses-responsible-ai-measures-for-global-militaries/

