---
order: 7
eyebrow: Verification
title: AI in Verification
intro: 'Artificial Intelligence has proven to be a powerful tool for analyzing large volumes of data, making it an important asset for future arms control verification regimes. This chapter explores potential consequences of AI for verification. We find that the availability of proper training data for verification purposes can pose a challenge that needs to be addressed by AI engineers building models for verification. The explainability of AI results – i.e. the reasoning behind outputs – must be addressed by developers and practitioners to ensure sound verification assessments and to provide inspectors insights into the potential and limits of decision-supporting AI models. To create trust and acceptance for AI-aided verification and monitoring, AI verification systems should be developed and tested jointly with all treaty parties.  Training data and models should be shared among parties, where possible, to foster transparency and enable independent validation.'
category: Focus
tags:
-	CTBT
-	CTBTO
-	IAEA
-	Inspection
-	Nuclear Weapons
-	Monitoring
-	Open-Source Data
-	OSINT
-	Radioactivity
-	Radionuclide
-	Satellites
-	Social Media
-	Verification

authors:
  - f-unruh-en
  - m-goettsche-en25
---

<Aside>Für eine kurze Einführung zum Umfassenden Kernwaffen-Teststoppvertrag (CTBT) siehe den [Anhang](/2025/vertrage-und-instrumente-im-zusammenhang-mit-rustungskontrolle-abrustung-und-nichtverbreitung-von-kernwaffen/)</Aside>

<Leadin>In den letzten Jahren</Leadin> ist die Menge an Daten, die für die Verifikation von Nichtverbreitungs- und Rüstungskontrollverpflichtungen sowie für die Überwachung einschlägiger Aktivitäten zur Verfügung stehen, kontinuierlich gestiegen. Dies betrifft zum einen Datenquellen, die bereits heute für solche Zwecke genutzt werden. Es ist eine Zunahme der Verwendung verschiedener Messsensoren zu beobachten, insbesondere für neuartige Fernüberwachungsanwendungen. Die Internationale AtomenergieOrganisation (IAEO) betreibt beispielsweise mehr als 1.300 Beobachtungskameras, die dazu beitragen, die Zahl der Inspektionen vor Ort zu verringern und gleichzeitig das Wissen über den Verbleib des nuklearen Materials aufrechtzuerhalten.<Footnote number="1" /> Die Organisation des Vertrags über ein umfassendes Verbot von Nuklearversuchen (CTBTO) betreibt Überwachungsstationen auf der ganzen Welt, um verbotene Atomwaffentests aufzudecken. Daten, wie z. B. seismische oder Radionuklid-Messungen, werden von 321 Stationen kontinuierlich aufgezeichnet, und die Datenströme werden ständig ausgewertet.<Footnote number="2" /> Außerdem können kommerzielle Satelliten mindestens alle paar Stunden jeden beliebigen Punkt auf der Erde abbilden.<Footnote number="3" /> Andererseits stellt sich die Frage, inwieweit die überwältigende Menge an öffentlich zugänglichen Informationen – auch aus den sozialen Medien – in Zukunft zu Verifikations- und Überwachungszwecken genutzt werden könnte. Dazu gehören heterogene Daten wie Text, Bilder, Audio und Video.

<Aside>Sensoren, Satelliten, soziale Medien: Die Verfügbarkeit von Daten steigt.</Aside>

Die Analyse der wachsenden Datenmengen, die in den heutigen Verifikations- und Überwachungskontexten verwendet werden, ist angesichts der begrenzten Anzahl von Inspektor\*innen und Analyst\*innen bereits eine anspruchsvolle Aufgabe. Die Ausweitung der Datenmenge, die für die Verifikation und Überwachung verwendet wird, erfordert daher computergestützte Unterstützung. Dies ist insbesondere dann der Fall, wenn die Daten heterogen, unstrukturiert und komplex sind. Dieses Kapitel befasst sich mit KI als Werkzeug für die Analyse von Daten im Kontext von internationaler Sicherheit und Frieden. Kann sie ein zuverlässiges und vertrauenswürdiges Verifikations- und Überwachungsinstrument werden?

<Aside>Im Anhang findet sich eine [Liste von Beispielen für KI-Anwendungen in Kontexten von Monitoring und Verifikation](/2025/beispiele-fur-ki-anwendungen-in-kontexten-von-monitoring-und-verifikation/).</Aside>

KI wird wahrscheinlich für mehrere Bereiche der Rüstungskontrolle relevant werden.<Footnote number="4" /> Verschiedene Bereiche nutzen gemeinsame Datenquellen, wie z. B. Satellitenbilder, und haben ähnliche Herausforderungen bei der Datenanalyse. Neben den derzeitigen vertraglichen Verifikationsregimen können KI-Anwendungen auch in weiteren Bereichen wie der Überwachung von Waffenstillständen, friedenssichernden Maßnahmen oder der gesellschaftlichen Verifikation mittels Daten aus öffentlich zugänglichen Quellen von Bedeutung sein. In der Tat stammen einige der ersten KI-Anwendungen aus diesen Bereichen.

## Verifikationsrelevante Daten
Daten können sowohl mit der als auch ohne die Absicht erzeugt werden, sie zur Verifikation zu verwenden. Instrumentengestützte Daten – wie z. B. technische Messoder Überwachungsdaten – wurden schon immer für Inspektionen verwendet, aber Umfang und Volumen solcher Daten haben mit der Einführung von neuen Fernüberwachungsansätzen zugenommen. Solche Daten dienen in erster Linie dazu, die Einhaltung von Rüstungskontrollverpflichtungen zu bewerten. Die Erfassung von Messdaten kann in Laboratorien und anderen kontrollierten Umgebungen getestet und angepasst werden, und Messgeräte können für diesen Zweck entworfen und geeicht werden.

<Aside>Menschliche Kommunikationsdaten können wertvolle verifikationsrelevante Informationen enthalten, sind aber schwer zu analysieren.</Aside>

Andererseits sind in letzter Zeit immer mehr Daten aus öffentlich zugänglichen Quellen über eine Reihe von Medien verfügbar geworden. Obwohl der Zweck dieser Daten ursprünglich nicht mit der Verifikation zusammenhängt, können sie dennoch für diesen Zweck nützliche Informationen enthalten. Die Daten, in der Regel in Form von Text-, Bild-, Audio- und Videodateien, sind oft im Internet frei zugänglich – z. B. in Form von Beiträgen in sozialen Medien – oder müssen digitalisiert werden – z. B. Zeitungsartikel oder Radioübertragungen. Da diese Daten fast überall auf der Welt produziert werden, können sie für Verifikation relevant sein, da sie nicht deklarierte Anlagen oder Aktivitäten aufdecken können. Die Daten sind zwar leicht verfügbar, aber unstrukturiert, und die Herkunft der Daten stellt die Analyse vor mehrere Herausforderungen, z. B. unterschiedliche Sprachen und Datenformate, außergewöhnlich große Mengen und begrenzte Möglichkeiten zur Überprüfung der Glaubwürdigkeit.

<Aside>Öffentlich zugängliche Informationen können die Einhaltung von Verpflichtungen und Verträgen nicht beurteilen, können aber als Überwachungsinstrument für die formale Verifikation nützlich sein.</Aside>

Es muss betont werden, dass derzeit nicht bekannt ist, wie relevant öffentlich zugängliche Informationen für Verifikationszwecke sind. Umfassende Überprüfungen dieser Daten könnten als exzessiv angesehen werden und Fragen der Verhältnismäßigkeit aufwerfen. Sie würden höchstwahrscheinlich nicht als Grundlage für die Beurteilung der Einhaltung von Verpflichtungen akzeptiert werden, sondern eher für weniger formale Überwachungszwecke. Ihr Zweck könnte darin bestehen, Hinweise zu erhalten, die dann formale Verifikationsaktivitäten auslösen könnten, z. B. indem sie dabei helfen, zu bestimmen, wann und wo Vor-Ort-Inspektionen durchgeführt werden sollen. Wenn sie wirksam sind, könnten solche Verfahren möglicherweise die Zahl der Inspektionen vor Ort reduzieren

## KI-gestütze Datenanalyse

<Aside>Diese KI-Anwendung wird auch im [Kapitel von Carmen García López](/2025/ki-entwicklungen-in-der-chemie/) diskutiert.</Aside>

KI ist besonders geeignet, um mit großen und vielfältigen Datensätzen umzugehen, und kann Muster und Zusammenhänge erkennen. Es gibt mehrere KI-basierte Analysen, die an Verifikations- oder Überwachungsaktivitäten angepasst werden könnten. So gibt es beispielsweise Ansätze, bei denen KI verwendet wird, um Beiträge in sozialen Medien in Krisenszenarien zu gruppieren und zu kennzeichnen,<Footnote number="5" /> die angepasst werden könnten, um Waffenstillstandsverletzungen zu identifizieren oder steigende Spannungen und bevorstehende Zusammenstöße zwischen bewaffneten Gruppen zu erkennen. In einer Studie im nuklearen Bereich wurde die Fähigkeit von KI-Systemen, verschiedene Formen von Daten gleichzeitig zu analysieren, genutzt, um Texte, Bilder, Audio- und Videodateien zu scannen, mit dem Ziel die Weiterverbreitung von Nukleartechnik zu ermitteln, indem die KI abgefragt wurde, Daten im Zusammenhang mit Geräten zur Urananreicherung zu identifizieren.<Footnote number="6" /> Eine andere Studie entwickelte eine vor Ort einsetzbare KI zur Identifizierung chemischer Kampfstoffe mit kolorimetrischen Sensoren.<Footnote number="7" /> Weitere KI-Anwendungen für die Verifikation werden im Anhang zu diesem Kapitel beschrieben.

<Aside>Die Zielsetzungen der angewandten KI können unterschiedlich sein: Auswahl potenziell relevanter Daten, die von Menschen überprüft werden sollen, oder Ableitung direkter probabilistischer Bewertungen.</Aside>

Als Werkzeug der Verifikation muss der Output einer implementierten KI mit einem eindeutigem Verifikationsziel verknüpft sein. Wir unterscheiden zwischen selektiven und probabilistischen Outputs von KI-Anwendungen. Ein selektiver Output ist idealerweise eine bestimmte Teilmenge der Eingabedaten, die verifikationsrelevante Informationen enthalten könnte, wie z. B. Indikatoren für die Nichteinhaltung von Verpflichtungen. Menschliche Analyst\*innen müssen die tatsächliche Bedeutung der identifizierten Daten überprüfen und bewerten. Es ist nicht vertretbar, dass die KI endgültige Schlussfolgerungen zieht, sondern sie sollte stattdessen menschlichen Inspektor\*innen dabei helfen, sich auf die konkreten Daten zu konzentrieren, die ihre Bewertungen beeinflussen. Eine KI könnte zum Beispiel Bilder identifizieren, die vertraglich verbotene Gegenstände zeigen, oder verdächtige Aktivitäten hervorheben. Im Zusammenhang mit Sensordaten ist ein probabilistischer Output eine direkte Bewertung, d. h. eine abgeleitete Größe. Die betreffende KI könnte z. B. den Ort bestimmen, von dem aus eine Granate während eines Waffenstillstands abgefeuert wurde, oder die Wahrscheinlichkeit, dass ein bestimmter Vorgang stattfindet. In diesen Fällen übernimmt die KI eine direktere Rolle beim Ziehen von Schlussfolgerungen.

<Figure src="assets/selective-outputs_EN.jpg" size="large" alt="A diagram illustrating the concepts of selective and probabilistic outputs of Artificial Intelligence (AI) in verification processes.
On the left side, the diagram shows AI with selective output, which identifies specific input data (e.g., a tank, a satellite, or a group of people) from a set of input sources.
On the right side, the diagram shows AI with probabilistic output, which evaluates input data and provides probabilities for different scenarios. Examples include:
- The presence of a warhead with a probability of 87.5%
- The presence of granules near a specific location with a probability of 34.1%
-	The transfer of weapons through company A with a probability of 91.7%
The diagram also includes visual representations of data types such as spectra, seismograms, and tables with results.
The explanatory text at the bottom states that the output of AI in verification is associated with a specific goal. AI with selective output identifies a specific subset of input data that should be evaluated by human analysts. AI with probabilistic output provides a further evaluation, indicating the probability of a specific scenario based on the input data.
A challenge is the AI's training data, which includes diverse datasets: not only 'perfect' data, but also real-world scenarios, such as incomplete backgrounds, varying lighting conditions, or different angles." />

## Herausforderungen

### Trainingsdaten
Eine wichtige Herausforderung sind die Trainingsdaten für die KI. Die Qualität, Vielfalt und Unvoreingenommenheit des Trainingsdatensatzes ist ein entscheidender Faktor für die Qualität des Modells. Im Allgemeinen müssen die Trainingsdatensätze hinreichend groß sein und genügend Daten enthalten, die den zu erwartenden realen Szenarien unter Beobachtung ähnlich sind. Wenn eine KI beispielsweise die Aufgabe hat, Bilder von verschiedenen Waffen zu erkennen, sollte die Trainingsmenge unterschiedliche Bilder enthalten – z. B. verschiedene Winkel, Beleuchtungen, Hintergründe und unvollständige oder veränderte Objekte. Die Beschaffung dieser Art von Daten kann ein mühsames Unterfangen sein und wird sogar noch schwieriger, wenn ständig aktualisierte Datensätze erforderlich sind, um sich an dynamische Entwicklungen anzupassen. Die erforderlichen Daten können spärlich oder gar nicht vorhanden sein. Es ist möglich, die Größe des Datensatzes zu erhöhen, indem man bestehende Daten ergänzt<Footnote number="8" /> oder Datensätze mit synthetischen, computergenerierten Daten erstellt. Ein auf solchen Daten trainiertes Modell könnte unter Umständen jedoch nicht robust sein, d. h. bei der Anwendung auf reale Daten schlecht abschneiden. Sowohl reale als auch computergenerierte Trainingsdaten können mit inhärenten Verzerrungen behaftet sein, die von KI-Modellen bei ihrem Training übernommen werden können.

<Figure src="assets/training-data_EN.jpg" size="large" alt="Illustration mit der Überschrift: „Herausforderung: Trainingsdaten“. Illustration zeigt Piktogramm eines Panzers mit dem Text „Diverse Datensätze: nicht nur ‚perfekte‘ Daten…“, daneben steht der Text „…sondern auch reale Szenarien, z. B.“ und darunter jeweils ein Begriff und ein Piktogramm. „Unvollständig“ mit einem Piktogramm, das einen Teil eines Panzers und einen Teil eines Baums zeigt; „Hintergründe“ mit einem Piktogramm, das einen Panzer und davor und dahinter Personen zeigt; „Lichtverhältnisse“ mit einem Piktogramm, das einen Panzer auf sehr dunklem Untergrund mit wenig Kontrast zeigt; „anderer Winkel“ mit einem Piktogramm, das einen Panzer von oben zeigt; „verändert“ mit einem Piktogramm, das einen Panzer mit anderen Proportionen zeigt." />

Another challenge primarily associated with human communication data is unlabeled data. This consists of inputs without accompanying descriptions or classifications, making it harder for AI to learn what each input represents. For instance, social media images usually do not have labels of what is visible in the picture. Manually labeling this data can be prone to errors and would be tedious, so technical approaches would need to be leveraged, such as using a small manually labelled data set to “fine-tune” a model that has already been trained on a similar task, i.e. “transfer learning”.<Footnote number="9" /> All these means to tackle challenges of training data sets can result in unexpected AI outcomes when applied to real-world data, thus validating AI results with real-world data is an important part of the development process.

### Confidence and Explainability
The explainability of AI results is a general issue, but is especially important in verification. Unlike “traditional” analysis techniques, AI models perform calculations with millions of fine-tuned parameters, rendering it difficult or impossible to comprehend which data features were most significant for determining the results. Can one be sure, for instance, that the selected data shown to analysts is representative? Can we be sure that no data is filtered out that could give counter-indications? In a verification scenario, especially in cases of alleged non-compliance, it is imperative to be able to explain how inspectorates come to their final conclusions. In formal arms control settings, mutual confidence in the validity of the results of the AI must exist before it can be incorporated into a verification regime. Even in informal settings, international acceptance of assessments involving AI models is contingent on being confident in the AI’s reliability.

<Aside>How well AI results can be explained is a decisive factor for its acceptance in international agreements.</Aside>

There are several possibilities towards addressing this issue. First, explainable AI (“XAI”) approaches should be leveraged to bring some light into the “black box” of AI. The XAI field has grown significantly over the past years and techniques such as “attribution”, which highlights the most important aspects of the input data for the AI’s output, have been developed.<Footnote number="10" /> Nevertheless, many open research questions remain in this field; as of today, XAI cannot address all explainability issues.

Second, research and development of AI, especially models and training data, must be as transparent as possible to all parties to foster trust in the AI’s results and therefore build the necessary basis for AI to be an accepted asset in international agreements. Verification regimes would require codified procedures regarding data collection, training data selection, AI model design and training, testing and validation. In order to generate the necessary trust in the AI models by all parties, a generally high level of transparency is required. At best, this entails sharing training data and models, as well as joint international development of AI across all stages. Also, insights about shortcomings and limitations of the model and potential failures need to be disclosed. If this is not ensured, results by the AI are prone to being rejected by the parties, or may even make AI an exploitable weak spot, allowing parties to question the reliability of the verification regime in general.

However, some stakeholders may express reservations against comprehensive transparency. States may refuse to share data deemed sensitive or dual-use relevant. Fully transparent AI models could enable adversarial attacks, which are subtle almost incomprehensible changes of input data that are able to fool the model. The concerns regarding data confidentiality and integrity of AI models must be subject to debate before AI is considered part of verification regimes. Nevertheless, any solution emerging from such discussion cannot discard the maxim of far-reaching transparency – otherwise, AI is unlikely to be regarded as an accepted and trusted technique for verification.

### Humans and Machines in Decision Making Processes
The introduction of AI into verification processes inevitably cedes parts of the decision-making to the AI system, even where the AI merely assists inspectors, who continue to draw their own conclusions. Human analysts have limited control over the calculation and reasoning of the AI, which however produces information on which analysts rely. This is true both for probabilistic and selective outputs, in which analysts only see the partial data which may be biased. All AI approaches share the consequence that the final assessment made by humans is not based on the complete original data set presented to the AI, but rather on a pre-processed and filtered one.

<Aside>Employing AI in verification inevitably changes the role of humans in the decision-making process. The human-computer interaction must be designed with accountability in mind.</Aside>

To use AI responsibly and accountably in verification, questions about the roles of and interactions between humans and machines must be the subject of discussion. For instance, there is already a discourse about ethical AI guidelines for applications in IAEA safeguards.<Footnote number="11" /> Even though humans may make a final decision, trust in the decision-supporting AI has been identified as a crucial issue in general.<Footnote number="12" /> When humans review or analyze data directly, they can profit from expert intuition and contextual insights. When AI takes over the data processing step, these “soft” judgement capabilities are difficult to maintain.

### Conclusion

AI techniques will very likely play a role for future verification and monitoring. The amount of data is increasing rapidly in availability and has already been successfully analyzed with AI in other contexts, making AI an indispensable tool. Trust in the AI tools used during verification is crucial because drawing conclusions about compliance is the goal of verification and is not only a technical but also a political process. This is particularly relevant, given that the use of AI in verification necessarily means that inspectorates have less independent control.

Approaches to deal with this and build trust in AI methods are technical and procedural in nature. On the technical side, XAI techniques can be applied, which can to some extent explain how input information resulted in a specific assessment. Procedurally, AI for verification must be planned, developed, and tested in a transparent manner to build confidence in the AI’s results, preferably in international field exercises using real-world data for validation. At the same time, a dialogue on handling sensitive data and ensuring model integrity is necessary to enable openness for comprehensive transparency. This debate must be conducted jointly, involving all stakeholders and prior to the incorporation of AI into any verification regime.

Stakeholders should allocate resources now, as AI capabilities advance rapidly and building sufficient confidence in AI technology will require a considerable effort over a long time. Because catching up later may prove difficult, states that wish to shape future international security agreements must engage early.

<FootnoteList notes={[
<>IAEA. (2023). IAEA Annual Report 2023. <a href="https://www.iaea.org/sites/default/files/gc/gc68-2.pdf">https://www.iaea.org/sites/default/files/gc/gc68-2.pdf</a>; Smartt, H. (2022). Remote Monitoring Systems/Remote Data Transmission for International Nuclear Safeguards. Sandia National Laboratories (SNL). <a href="https://doi.org/10.2172/1862624">https://doi.org/10.2172/1862624</a></>,
<>CTBTO. (n.d.). International Data Centre. Abgerufen am 31. Juli, 2025, unter <a href="https://www.ctbto.org/our-work/internationaldata-centre">https://www.ctbto.org/our-work/internationaldata-centre</a>; CTBTO. (n.d.). The International Monitoring System. Abgerufen am 31. Juli 2025, unter <a href="https://www.ctbto.org/our-work/international-monitoring-system">https://www.ctbto.org/our-work/international-monitoring-system</a></>,
<>Moric, I. (2022). Capabilities of Commercial Satellite Earth Observation Systems and Applications for Nuclear Verification and Monitoring. Science & Global Security, 30(1), 22–49. doi:10.1080/08929882.2022.2063334</>,
<>Reinhold, T., & Schörnig, N. (Hrsg.). (2024). Armament, Arms Control and Artificial Intelligence. Springer Verlag. <a href="https://doi.org/10.1007/978-3-031-11043-6">https://doi.org/10.1007/978-3-031-11043-6</a></>,
<>Bayer, M., Kaufhold, M.-A., & Reuter, C. (2021). Information Overload in Crisis Management: Bilingual Evaluation of Embedding Models for Clustering Social Media Posts in Emergencies. ECIS 2021 Research Papers.; PEASEC. (n.d.). Open Data Observatory. Abgerufen am 27. Mai 2025, unter <a href="https://peasec.de/projects/observatory/">https://peasec.de/projects/observatory/</a></>,
<>Feldman, Y., Arno, M., Carrano, C., Ng, B., & Chen, B. (2018). Toward a Multimodal-Deep Learning Retrieval System for Monitoring Nuclear Proliferation Activities. Journal of Nuclear Materials Management, 46(3). <a href="https://www.ingentaconnect.com/content/inmm/jnmm/2018/00000046/00000003/art00008">https://www.ingentaconnect.com/content/inmm/jnmm/2018/00000046/00000003/art00008</a></>,
<>Bae, S., Kang, K., Kim, Y. K., Jang, Y. J., & Lee, D.-H. (2025). Field-Deployable Real-Time AI System for Chemical Warfare Agent Detection Using YOLOv8 and Colorimetric Sensors. Chemometrics and Intelligent Laboratory Systems, 261, 105365. <a href="https://doi.org/10.1016/j.chemolab.2025.105365">https://doi.org/10.1016/j.chemolab.2025.105365</a></>,
<>Shorten, C., & Khoshgoftaar, T. M. (2019). A Survey on Image Data Augmentation for Deep Learning. Journal of Big Data, 6(1), 60. <a href="https://doi.org/10.1186/s40537-019-0197-0">https://doi.org/10.1186/s40537-019-0197-0</a></>,
<>Pan, S. J., & Yang, Q. (2010). A Survey on Transfer Learning. IEEE Transactions on Knowledge and Data Engineering, 22(10), 1345–59. <a href="https://doi.org/10.1109/TKDE.2009.191">https://doi.org/10.1109/TKDE.2009.191</a>; Zoph, B., Ghiasi, G., Lin, T.-Y., Cui, Y., Liu, H., Cubuk, E. D., & Le, Q. V. (2020). Rethinking Pre-Training and Self-Training. arXiv.org. <a href="https://arxiv.org/abs/2006.06882v2">https://arxiv.org/abs/2006.06882v2</a></>,
<>Linardatos, P., Papastefanopoulos, V., & Kotsiantis, S. (2021). Explainable AI: A Review of Machine Learning Interpretability Methods. Entropy, 23(1), 18. <a href="https://doi.org/10.3390/e23010018">https://doi.org/10.3390/e23010018</a>; Longo, L., Brcic, M., Cabitza, F., Choi, J., Confalonieri, R., Del Ser, J., Guidotti, R., Hayashi, Y., Herrera, F., Holzinger, A., Jiang, R., Khosravi, H., Lecue, F., Magieri, G., Páez, A., Samek, W., Schneider, J., Speith, T., & Stumpf, S. (2024). Explainable Artificial Intelligence (XAI) 2.0: A Manifesto of Open Challenges and Interdisciplinary Research Directions. Information Fusion, 106, 102301. <a href="https://doi.org/10.1016/j.inffus.2024.102301">https://doi.org/10.1016/j.inffus.2024.102301</a>; Lopes, P., Silva, E., Braga, C., Oliveira, T., & Rosado, L. (2022). XAI Systems Evaluation: A Review of Human and ComputerCentred Methods. Applied Sciences, 12(19), 9423. <a href="https://doi.org/10.3390/app12199423">https://doi.org/10.3390/app12199423</a>; Simonyan, K., Vedaldi, A., & Zisserman, A. (2014). Deep Inside Convolutional Networks: Visualising Image Classification Models and Saliency Maps. arXiv.org. <a href="https://doi.org/10.48550/arXiv.1312.6034">https://doi.org/10.48550/arXiv.1312.6034</a></>,
<>Murphy, C., & Barr, J. (2023). Responsibly Harnessing the Power of AI. Consolidated Nuclear Security, LLC. <a href="https://www.osti.gov/biblio/2283006">https://www.osti.gov/biblio/2283006</a></>,
<>Bayer, S., Gimpel, H., & Markgraf, M. (2022). The Role of Domain Expertise in Trusting and Following Explainable AI Decision Support Systems. Journal of Decision Systems 32(1), 110–38. doi. <a href="https://doi.org/10.1080/12460125.2021.1958505">https://doi.org/10.1080/12460125.2021.1958505</a></>
]} />
