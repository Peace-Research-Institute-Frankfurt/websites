---
title: Red teaming (in the AI context)
term_id: red-teaming
---

An interactive method used to test AI models, with the aim of preventing harmful behavior such as the leakage of sensitive data and the generation of toxic, biased, or factually inaccurate content.
